from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import sys
import os

# Ensure the script directory is included in Python's path
sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))

from preprocessing import preprocessing_json_from_directory
from webscraper import WebScraper
from database import upload_preprocessed_files_to_vector_store

# Define the default arguments for the DAG
default_args = {
    'owner': 'airflow',  # Owner of the DAG
    'depends_on_past': False,  # Does not depend on previous DAG runs
    'email_on_failure': False,  # Disable email notifications on failure
    'email_on_retry': False,  # Disable email notifications on retry
    'retries': 1,  # Number of retries in case of failure
    'retry_delay': timedelta(minutes=5),  # Time to wait between retries
}

# Create the DAG
with DAG(
    'preprocessing_and_webscraping_dag',
    default_args=default_args,
    description='DAG to run web scraping and process a JSON file',  # Description of the DAG
    schedule_interval=timedelta(days=1),  # Schedule interval (daily)
    start_date=datetime(2024, 9, 11),  # Start date for the DAG
    catchup=False,  # Do not run past DAG instances
) as dag:

    # Function to run the web scraper
    def run_webscraper():
        """
        This function initializes and runs the web scraper on the specified URL.
        The web scraper is set to verbose mode for debugging, and the scraped content
        is saved as a JSON file.
        """
        url = "https://vitalik.eth.limo/"
        scraper = WebScraper(url, verbose=True, save_to_file=True, save_format='json')  # Enable verbose mode for debugging
        scraper.scrape()  # Run the web scraper

    # Task to execute the web scraper
    scrape_task = PythonOperator(
        task_id='run_webscraper',  # Task ID for the web scraper
        python_callable=run_webscraper,  # Python function to be executed
    )

    # Function to preprocess the scraped JSON files and save as CSV
    def preprocessing_and_save():
        """
        This function processes the most recent JSON file generated by the web scraper.
        It transforms the JSON content into a CSV format and saves it in the specified directory.
        """
        webscraping_dir = '/opt/airflow/dags/files/webscraper/'  # Directory where web scraping output is stored
        preprocessed_dir = '/opt/airflow/dags/files/preprocessed/'  # Directory to save preprocessed output
        preprocessing_json_from_directory(webscraping_dir, preprocessed_dir)  # Run the preprocessing function

    # Task to run preprocessing
    run_preprocessing = PythonOperator(
        task_id='run_preprocessing',  # Task ID for preprocessing
        python_callable=preprocessing_and_save,  # Python function to be executed
    )

    # Task to upload files to the vector store (now coming from database.py)
    def upload_preprocessed_files():
        """
        This function uploads the preprocessed files (in CSV format) to the vector store.
        The files to be uploaded are located in the preprocessed directory.
        """
        upload_preprocessed_files_to_vector_store()  # Call the function to upload files

    upload_files_task = PythonOperator(
        task_id='upload_preprocessed_files',  # Task ID for uploading files to the vector store
        python_callable=upload_preprocessed_files,  # Python function to be executed
    )

    # Task dependencies: run web scraper first, then preprocessing, and finally upload
    scrape_task >> run_preprocessing >> upload_files_task